{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cf3477e5-ac2b-4cf8-9de3-e95782130327",
      "metadata": {
        "scrolled": true,
        "id": "cf3477e5-ac2b-4cf8-9de3-e95782130327",
        "outputId": "2b816510-aee1-44d0-a60f-057c3f9d6029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CML_processing_by_ML'...\n",
            "remote: Enumerating objects: 143, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
            "remote: Total 143 (delta 79), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (143/143), 9.28 MiB | 10.69 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n"
          ]
        }
      ],
      "source": [
        "# Local clone\n",
        "! git clone https://github.com/nanopiero/CML_processing_by_ML.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from os.path import join, isdir, isfile\n",
        "from os import listdir as ls\n",
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "sys.path.append('CML_processing_by_ML')\n",
        "from src.utils.simulation import create_dataloader\n",
        "import src.utils.architectures_fcn\n",
        "from src.utils.architectures import load_archi\n",
        "from src.utils.architectures_fcn import UNet_causal_5mn_atrous, UNet_causal_5mn_atrous_rescale"
      ],
      "metadata": {
        "id": "0u1PXBYMSD-f"
      },
      "id": "0u1PXBYMSD-f",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary with pseudo \"distances\" (distances between two antennnas) for 1,000 pseudo CML ids.\n",
        "idx2distance = {i: 0.2 +  1.8 * torch.rand((1,)).item() for i in range(0, 1000)}\n",
        "duration = 4096  # Duration for each tensor pair\n",
        "batch_size = 100  # Number of samples per batch\n",
        "dataloader = create_dataloader(duration, idx2distance, batch_size)"
      ],
      "metadata": {
        "id": "iywu4Uq-e2rx"
      },
      "id": "iywu4Uq-e2rx",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "789fdbdd-33b7-4447-a26a-cc4a4ff3a10d",
      "metadata": {
        "id": "789fdbdd-33b7-4447-a26a-cc4a4ff3a10d"
      },
      "outputs": [],
      "source": [
        "# Here we samples 64 ground-truth rainy processes and their noisy counterpart\n",
        "# A rainy process is modeled by a 1-d Neymann-Scott process\n",
        "# The Intensity of the Poisson process for parent events is 0.05 x distance\n",
        "# The resulting rainy process is divided by the distance to give the \"ground truth\"\n",
        "# while it is corrupted through the following steps to yield \"noisy_series\":\n",
        "# - applying a non linear conversion to an attenuation in db\n",
        "# - applying a \"wet antenna convolution filter\" (kind of sliding mean)\n",
        "# - adding a high (gaussian noise with non linear dependance of sigma wrt the intensity)\n",
        "# - adding a low frequency random processes\n",
        "\n",
        "for batch_idx, (idxs, dists, ground_truths, noisy_series) in enumerate(dataloader):\n",
        "  if batch_idx == 0:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "sigma = 2\n",
        "for k in range(5):\n",
        "  print(idxs[k], dists[k])\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(np.arange(duration), ground_truths[k], label='ground_truth')\n",
        "  plt.plot(np.arange(duration), noisy_series[k], label='predictor')\n",
        "  plt.title(f'Inputs and Targets for the CML nÂ°{idxs[k].item():.0f} (ditance: {dists[k].item():.2f})')\n",
        "  plt.xlabel('Time (minutes)')\n",
        "  plt.ylabel('Event Density')\n",
        "  plt.ylim(-1,6)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TUb6dcqOQNcp"
      },
      "id": "TUb6dcqOQNcp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "92ab6b68-0814-416f-9c23-85dfa0b7bba8",
      "metadata": {
        "id": "92ab6b68-0814-416f-9c23-85dfa0b7bba8",
        "outputId": "167592c2-f855-42d0-a61c-8c413ad875ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5791\n",
            "Epoch [2/50], Loss: 0.2847\n",
            "Epoch [3/50], Loss: 0.2560\n",
            "Epoch [4/50], Loss: 0.2368\n",
            "Epoch [5/50], Loss: 0.2300\n",
            "Epoch [6/50], Loss: 0.2173\n",
            "Epoch [7/50], Loss: 0.1978\n",
            "Epoch [8/50], Loss: 0.1868\n",
            "Epoch [9/50], Loss: 0.1745\n",
            "Epoch [10/50], Loss: 0.1711\n",
            "Epoch [11/50], Loss: 0.1632\n",
            "Epoch [12/50], Loss: 0.1597\n",
            "Epoch [13/50], Loss: 0.1567\n",
            "Epoch [14/50], Loss: 0.1555\n",
            "Epoch [15/50], Loss: 0.1552\n",
            "Epoch [16/50], Loss: 0.1510\n",
            "Epoch [17/50], Loss: 0.1486\n",
            "Epoch [18/50], Loss: 0.1492\n",
            "Epoch [19/50], Loss: 0.1490\n",
            "Epoch [20/50], Loss: 0.1500\n",
            "Epoch [21/50], Loss: 0.1485\n",
            "Epoch [22/50], Loss: 0.1473\n",
            "Epoch [23/50], Loss: 0.1449\n",
            "Epoch [24/50], Loss: 0.1446\n",
            "Epoch [25/50], Loss: 0.1474\n",
            "Epoch [26/50], Loss: 0.1474\n",
            "Epoch [27/50], Loss: 0.1470\n",
            "Epoch [28/50], Loss: 0.1441\n",
            "Epoch [29/50], Loss: 0.1448\n",
            "Epoch [30/50], Loss: 0.1452\n",
            "Epoch [31/50], Loss: 0.1448\n",
            "Epoch [32/50], Loss: 0.1443\n",
            "Epoch [33/50], Loss: 0.1442\n",
            "Epoch [34/50], Loss: 0.1410\n",
            "Epoch [35/50], Loss: 0.1425\n",
            "Epoch [36/50], Loss: 0.1465\n",
            "Epoch [37/50], Loss: 0.1442\n",
            "Epoch [38/50], Loss: 0.1431\n",
            "Epoch [39/50], Loss: 0.1448\n",
            "Epoch [40/50], Loss: 0.1447\n",
            "Epoch [41/50], Loss: 0.1396\n",
            "Epoch [42/50], Loss: 0.1404\n",
            "Epoch [43/50], Loss: 0.1400\n",
            "Epoch [44/50], Loss: 0.1419\n",
            "Epoch [45/50], Loss: 0.1412\n",
            "Epoch [46/50], Loss: 0.1440\n",
            "Epoch [47/50], Loss: 0.1407\n",
            "Epoch [48/50], Loss: 0.1410\n",
            "Epoch [49/50], Loss: 0.1419\n",
            "Epoch [50/50], Loss: 0.1434\n"
          ]
        }
      ],
      "source": [
        "arch = \"UNet_causal_5mn_atrous\"\n",
        "nchannels = 1\n",
        "nclasses = 1 # Regression only\n",
        "dilation = 2\n",
        "atrous_rates=[6, 12, 18] #, 24, 30, 36, 42]\n",
        "additional_parameters = 0\n",
        "\n",
        "model = load_archi(arch, nchannels, nclasses, size=64, dilation=1,\n",
        "                   atrous_rates=atrous_rates, fixed_cumul=False,\n",
        "                   additional_parameters=additional_parameters)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# model = UNet(1, 1, 16).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "num_epochs = 50  # Adjust based on your needs\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (idxs, dists, ground_truths, noisy_series) in enumerate(dataloader):\n",
        "        inputs, targets = noisy_series.to(device), \\\n",
        "                          ground_truths.to(device)\n",
        "\n",
        "        # Add the channel's dim\n",
        "        inputs = inputs.unsqueeze(1)\n",
        "        targets = targets.unsqueeze(1)\n",
        "\n",
        "        # Zeroing gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f990b0-0257-4afc-8bf6-1f964b08367b",
      "metadata": {
        "id": "b0f990b0-0257-4afc-8bf6-1f964b08367b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_predictions(model, data_loader, num_samples=1):\n",
        "    model.eval()\n",
        "    L = 1000\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (idxs, dists, ground_truths, noisy_series) in enumerate(dataloader):\n",
        "\n",
        "            inputs = noisy_series.to(device).unsqueeze(1).float()  # Adjust input dimensions\n",
        "            outputs = model(inputs).cpu()\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                plt.figure(figsize=(16, 4))\n",
        "                # plt.plot(noisy_series[i].squeeze(), label='input')\n",
        "                plt.plot(ground_truths[i,:L].squeeze(), label='Observation')\n",
        "                plt.plot(outputs[i].squeeze()[:L], label='Prediction', linestyle='--')\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "            break  # Just show the first batch\n",
        "\n",
        "visualize_predictions(model, dataloader, num_samples=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to do better ?\n",
        "\n",
        "# An attempt with UNet_causal_5mn_atrous_rescale"
      ],
      "metadata": {
        "id": "tFGXzz6xh4ir"
      },
      "id": "tFGXzz6xh4ir",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arch = \"UNet_causal_5mn_atrous_rescale\"\n",
        "nchannels = 1\n",
        "nclasses = 1 # Regression only\n",
        "dilation = 2\n",
        "atrous_rates=[6, 12, 18] #, 24, 30, 36, 42]\n",
        "additional_parameters = 1005\n",
        "\n",
        "model = load_archi(arch, nchannels, nclasses, size=64, dilation=1,\n",
        "                   atrous_rates=atrous_rates, fixed_cumul=False,\n",
        "                   additional_parameters=additional_parameters)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# model = UNet(1, 1, 16).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "num_epochs = 50  # Adjust based on your needs\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (idxs, dists, ground_truths, noisy_series) in enumerate(dataloader):\n",
        "        inputs, targets = noisy_series.to(device), \\\n",
        "                          ground_truths.to(device)\n",
        "\n",
        "        # Add the channel's dim\n",
        "        inputs = inputs.unsqueeze(1)\n",
        "        targets = targets.unsqueeze(1)\n",
        "\n",
        "        # Zeroing gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        outputs, p = model(inputs, indices=idxs.to(device))\n",
        "        outputs[:,:,:] *= p[5:].view(outputs.shape[0],1,1)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')"
      ],
      "metadata": {
        "id": "ZenXWDE-7A9N",
        "outputId": "a0c41a19-659e-414c-e13d-59d91c0bb4f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZenXWDE-7A9N",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5577\n",
            "Epoch [2/50], Loss: 0.2818\n",
            "Epoch [3/50], Loss: 0.2562\n",
            "Epoch [4/50], Loss: 0.2376\n",
            "Epoch [5/50], Loss: 0.2377\n",
            "Epoch [6/50], Loss: 0.2321\n",
            "Epoch [7/50], Loss: 0.2256\n",
            "Epoch [8/50], Loss: 0.2189\n",
            "Epoch [9/50], Loss: 0.2123\n",
            "Epoch [10/50], Loss: 0.2086\n",
            "Epoch [11/50], Loss: 0.2061\n",
            "Epoch [12/50], Loss: 0.1832\n",
            "Epoch [13/50], Loss: 0.1675\n",
            "Epoch [14/50], Loss: 0.1666\n",
            "Epoch [15/50], Loss: 0.1564\n",
            "Epoch [16/50], Loss: 0.1531\n",
            "Epoch [17/50], Loss: 0.1501\n",
            "Epoch [18/50], Loss: 0.1470\n",
            "Epoch [19/50], Loss: 0.1508\n",
            "Epoch [20/50], Loss: 0.1459\n",
            "Epoch [21/50], Loss: 0.1489\n",
            "Epoch [22/50], Loss: 0.1472\n",
            "Epoch [23/50], Loss: 0.1466\n",
            "Epoch [24/50], Loss: 0.1449\n",
            "Epoch [25/50], Loss: 0.1444\n",
            "Epoch [26/50], Loss: 0.1439\n",
            "Epoch [27/50], Loss: 0.1428\n",
            "Epoch [28/50], Loss: 0.1429\n",
            "Epoch [29/50], Loss: 0.1386\n",
            "Epoch [30/50], Loss: 0.1386\n",
            "Epoch [31/50], Loss: 0.1377\n",
            "Epoch [32/50], Loss: 0.1383\n",
            "Epoch [33/50], Loss: 0.1400\n",
            "Epoch [34/50], Loss: 0.1395\n",
            "Epoch [35/50], Loss: 0.1382\n",
            "Epoch [36/50], Loss: 0.1374\n",
            "Epoch [37/50], Loss: 0.1371\n",
            "Epoch [38/50], Loss: 0.1367\n",
            "Epoch [39/50], Loss: 0.1358\n",
            "Epoch [40/50], Loss: 0.1362\n",
            "Epoch [41/50], Loss: 0.1376\n",
            "Epoch [42/50], Loss: 0.1378\n",
            "Epoch [43/50], Loss: 0.1377\n",
            "Epoch [44/50], Loss: 0.1352\n",
            "Epoch [45/50], Loss: 0.1357\n",
            "Epoch [46/50], Loss: 0.1358\n",
            "Epoch [47/50], Loss: 0.1379\n",
            "Epoch [48/50], Loss: 0.1371\n",
            "Epoch [49/50], Loss: 0.1364\n",
            "Epoch [50/50], Loss: 0.1356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arch = \"UNet_causal_5mn_atrous_complex_rescale\"\n",
        "nchannels = 1\n",
        "nclasses = 7\n",
        "dilation = 2\n",
        "atrous_rates=[6, 12, 18] #, 24, 30, 36, 42]\n",
        "additional_parameters = 0\n",
        "\n",
        "model = load_archi(arch, nchannels, nclasses, size=64, dilation=1,\n",
        "                   atrous_rates=atrous_rates, fixed_cumul=False,\n",
        "                   additional_parameters=additional_parameters)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# model = UNet(1, 1, 16).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "num_epochs = 300  # Adjust based on your needs\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (idxs, dists, ground_truths, noisy_series) in enumerate(dataloader):\n",
        "        inputs, targets = noisy_series.to(device), \\\n",
        "                          ground_truths.to(device)\n",
        "\n",
        "        use_first_network = torch.rand(idxs.shape, device=inputs.device) > 0.75\n",
        "        idxs[use_first_network] = -1\n",
        "        # Add the channel's dim\n",
        "        inputs = inputs.unsqueeze(1)\n",
        "        targets = targets.unsqueeze(1)\n",
        "\n",
        "        # Zeroing gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, idxs.to(device))\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')"
      ],
      "metadata": {
        "id": "82DZ0Eed9nsV",
        "outputId": "2c1659f7-d73b-4888-dd2e-b7593a395833",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "82DZ0Eed9nsV",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([100, 1, 4096])) that is different to the input size (torch.Size([100, 7, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Loss: 0.7794\n",
            "Epoch [2/300], Loss: 0.3845\n",
            "Epoch [3/300], Loss: 0.3147\n",
            "Epoch [4/300], Loss: 0.2921\n",
            "Epoch [5/300], Loss: 0.2807\n",
            "Epoch [6/300], Loss: 0.2640\n",
            "Epoch [7/300], Loss: 0.2468\n",
            "Epoch [8/300], Loss: 0.2345\n",
            "Epoch [9/300], Loss: 0.2313\n",
            "Epoch [10/300], Loss: 0.2294\n",
            "Epoch [11/300], Loss: 0.2214\n",
            "Epoch [12/300], Loss: 0.2126\n",
            "Epoch [13/300], Loss: 0.2038\n",
            "Epoch [14/300], Loss: 0.2062\n",
            "Epoch [15/300], Loss: 0.2026\n",
            "Epoch [16/300], Loss: 0.2028\n",
            "Epoch [17/300], Loss: 0.1997\n",
            "Epoch [18/300], Loss: 0.1965\n",
            "Epoch [19/300], Loss: 0.1923\n",
            "Epoch [20/300], Loss: 0.1927\n",
            "Epoch [21/300], Loss: 0.1944\n",
            "Epoch [22/300], Loss: 0.1926\n",
            "Epoch [23/300], Loss: 0.1885\n",
            "Epoch [24/300], Loss: 0.1863\n",
            "Epoch [25/300], Loss: 0.1883\n",
            "Epoch [26/300], Loss: 0.1864\n",
            "Epoch [27/300], Loss: 0.1896\n",
            "Epoch [28/300], Loss: 0.1879\n",
            "Epoch [29/300], Loss: 0.1828\n",
            "Epoch [30/300], Loss: 0.1835\n",
            "Epoch [31/300], Loss: 0.1816\n",
            "Epoch [32/300], Loss: 0.1792\n",
            "Epoch [33/300], Loss: 0.1826\n",
            "Epoch [34/300], Loss: 0.1771\n",
            "Epoch [35/300], Loss: 0.1785\n",
            "Epoch [36/300], Loss: 0.1805\n",
            "Epoch [37/300], Loss: 0.1791\n",
            "Epoch [38/300], Loss: 0.1771\n",
            "Epoch [39/300], Loss: 0.1775\n",
            "Epoch [40/300], Loss: 0.1752\n",
            "Epoch [41/300], Loss: 0.1758\n",
            "Epoch [42/300], Loss: 0.1763\n",
            "Epoch [43/300], Loss: 0.1766\n",
            "Epoch [44/300], Loss: 0.1724\n",
            "Epoch [45/300], Loss: 0.1742\n",
            "Epoch [46/300], Loss: 0.1714\n",
            "Epoch [47/300], Loss: 0.1726\n",
            "Epoch [48/300], Loss: 0.1738\n",
            "Epoch [49/300], Loss: 0.1715\n",
            "Epoch [50/300], Loss: 0.1761\n",
            "Epoch [51/300], Loss: 0.1712\n",
            "Epoch [52/300], Loss: 0.1726\n",
            "Epoch [53/300], Loss: 0.1722\n",
            "Epoch [54/300], Loss: 0.1702\n",
            "Epoch [55/300], Loss: 0.1705\n",
            "Epoch [56/300], Loss: 0.1653\n",
            "Epoch [57/300], Loss: 0.1684\n",
            "Epoch [58/300], Loss: 0.1668\n",
            "Epoch [59/300], Loss: 0.1683\n",
            "Epoch [60/300], Loss: 0.1661\n",
            "Epoch [61/300], Loss: 0.1650\n",
            "Epoch [62/300], Loss: 0.1670\n",
            "Epoch [63/300], Loss: 0.1683\n",
            "Epoch [64/300], Loss: 0.1637\n",
            "Epoch [65/300], Loss: 0.1681\n",
            "Epoch [66/300], Loss: 0.1637\n",
            "Epoch [67/300], Loss: 0.1630\n",
            "Epoch [68/300], Loss: 0.1668\n",
            "Epoch [69/300], Loss: 0.1640\n",
            "Epoch [70/300], Loss: 0.1616\n",
            "Epoch [71/300], Loss: 0.1634\n",
            "Epoch [72/300], Loss: 0.1643\n",
            "Epoch [73/300], Loss: 0.1601\n",
            "Epoch [74/300], Loss: 0.1614\n",
            "Epoch [75/300], Loss: 0.1598\n",
            "Epoch [76/300], Loss: 0.1600\n",
            "Epoch [77/300], Loss: 0.1598\n",
            "Epoch [78/300], Loss: 0.1613\n",
            "Epoch [79/300], Loss: 0.1606\n",
            "Epoch [80/300], Loss: 0.1570\n",
            "Epoch [81/300], Loss: 0.1563\n",
            "Epoch [82/300], Loss: 0.1566\n",
            "Epoch [83/300], Loss: 0.1575\n",
            "Epoch [84/300], Loss: 0.1555\n",
            "Epoch [85/300], Loss: 0.1561\n",
            "Epoch [86/300], Loss: 0.1552\n",
            "Epoch [87/300], Loss: 0.1558\n",
            "Epoch [88/300], Loss: 0.1550\n",
            "Epoch [89/300], Loss: 0.1560\n",
            "Epoch [90/300], Loss: 0.1535\n",
            "Epoch [91/300], Loss: 0.1559\n",
            "Epoch [92/300], Loss: 0.1543\n",
            "Epoch [93/300], Loss: 0.1533\n",
            "Epoch [94/300], Loss: 0.1554\n",
            "Epoch [95/300], Loss: 0.1549\n",
            "Epoch [96/300], Loss: 0.1528\n",
            "Epoch [97/300], Loss: 0.1532\n",
            "Epoch [98/300], Loss: 0.1533\n",
            "Epoch [99/300], Loss: 0.1530\n",
            "Epoch [100/300], Loss: 0.1509\n",
            "Epoch [101/300], Loss: 0.1506\n",
            "Epoch [102/300], Loss: 0.1510\n",
            "Epoch [103/300], Loss: 0.1524\n",
            "Epoch [104/300], Loss: 0.1504\n",
            "Epoch [105/300], Loss: 0.1500\n",
            "Epoch [106/300], Loss: 0.1496\n",
            "Epoch [107/300], Loss: 0.1481\n",
            "Epoch [108/300], Loss: 0.1479\n",
            "Epoch [109/300], Loss: 0.1493\n",
            "Epoch [110/300], Loss: 0.1510\n",
            "Epoch [111/300], Loss: 0.1478\n",
            "Epoch [112/300], Loss: 0.1461\n",
            "Epoch [113/300], Loss: 0.1478\n",
            "Epoch [114/300], Loss: 0.1472\n",
            "Epoch [115/300], Loss: 0.1467\n",
            "Epoch [116/300], Loss: 0.1489\n",
            "Epoch [117/300], Loss: 0.1482\n",
            "Epoch [118/300], Loss: 0.1451\n",
            "Epoch [119/300], Loss: 0.1465\n",
            "Epoch [120/300], Loss: 0.1480\n",
            "Epoch [121/300], Loss: 0.1454\n",
            "Epoch [122/300], Loss: 0.1449\n",
            "Epoch [123/300], Loss: 0.1431\n",
            "Epoch [124/300], Loss: 0.1434\n",
            "Epoch [125/300], Loss: 0.1452\n",
            "Epoch [126/300], Loss: 0.1432\n",
            "Epoch [127/300], Loss: 0.1477\n",
            "Epoch [128/300], Loss: 0.1473\n",
            "Epoch [129/300], Loss: 0.1432\n",
            "Epoch [130/300], Loss: 0.1462\n",
            "Epoch [131/300], Loss: 0.1445\n",
            "Epoch [132/300], Loss: 0.1439\n",
            "Epoch [133/300], Loss: 0.1414\n",
            "Epoch [134/300], Loss: 0.1424\n",
            "Epoch [135/300], Loss: 0.1433\n",
            "Epoch [136/300], Loss: 0.1405\n",
            "Epoch [137/300], Loss: 0.1413\n",
            "Epoch [138/300], Loss: 0.1437\n",
            "Epoch [139/300], Loss: 0.1424\n",
            "Epoch [140/300], Loss: 0.1431\n",
            "Epoch [141/300], Loss: 0.1412\n",
            "Epoch [142/300], Loss: 0.1423\n",
            "Epoch [143/300], Loss: 0.1421\n",
            "Epoch [144/300], Loss: 0.1408\n",
            "Epoch [145/300], Loss: 0.1410\n",
            "Epoch [146/300], Loss: 0.1422\n",
            "Epoch [147/300], Loss: 0.1418\n",
            "Epoch [148/300], Loss: 0.1414\n",
            "Epoch [149/300], Loss: 0.1400\n",
            "Epoch [150/300], Loss: 0.1385\n",
            "Epoch [151/300], Loss: 0.1435\n",
            "Epoch [152/300], Loss: 0.1432\n",
            "Epoch [153/300], Loss: 0.1402\n",
            "Epoch [154/300], Loss: 0.1391\n",
            "Epoch [155/300], Loss: 0.1407\n",
            "Epoch [156/300], Loss: 0.1414\n",
            "Epoch [157/300], Loss: 0.1387\n",
            "Epoch [158/300], Loss: 0.1397\n",
            "Epoch [159/300], Loss: 0.1407\n",
            "Epoch [160/300], Loss: 0.1389\n",
            "Epoch [161/300], Loss: 0.1381\n",
            "Epoch [162/300], Loss: 0.1377\n",
            "Epoch [163/300], Loss: 0.1417\n",
            "Epoch [164/300], Loss: 0.1380\n",
            "Epoch [165/300], Loss: 0.1373\n",
            "Epoch [166/300], Loss: 0.1397\n",
            "Epoch [167/300], Loss: 0.1386\n",
            "Epoch [168/300], Loss: 0.1379\n",
            "Epoch [169/300], Loss: 0.1395\n",
            "Epoch [170/300], Loss: 0.1376\n",
            "Epoch [171/300], Loss: 0.1373\n",
            "Epoch [172/300], Loss: 0.1370\n",
            "Epoch [173/300], Loss: 0.1392\n",
            "Epoch [174/300], Loss: 0.1388\n",
            "Epoch [175/300], Loss: 0.1406\n",
            "Epoch [176/300], Loss: 0.1387\n",
            "Epoch [177/300], Loss: 0.1362\n",
            "Epoch [178/300], Loss: 0.1374\n",
            "Epoch [179/300], Loss: 0.1376\n",
            "Epoch [180/300], Loss: 0.1360\n",
            "Epoch [181/300], Loss: 0.1396\n",
            "Epoch [182/300], Loss: 0.1371\n",
            "Epoch [183/300], Loss: 0.1356\n",
            "Epoch [184/300], Loss: 0.1349\n",
            "Epoch [185/300], Loss: 0.1362\n",
            "Epoch [186/300], Loss: 0.1358\n",
            "Epoch [187/300], Loss: 0.1362\n",
            "Epoch [188/300], Loss: 0.1386\n",
            "Epoch [189/300], Loss: 0.1391\n",
            "Epoch [190/300], Loss: 0.1372\n",
            "Epoch [191/300], Loss: 0.1382\n",
            "Epoch [192/300], Loss: 0.1364\n",
            "Epoch [193/300], Loss: 0.1361\n",
            "Epoch [194/300], Loss: 0.1347\n",
            "Epoch [195/300], Loss: 0.1366\n",
            "Epoch [196/300], Loss: 0.1351\n",
            "Epoch [197/300], Loss: 0.1354\n",
            "Epoch [198/300], Loss: 0.1338\n",
            "Epoch [199/300], Loss: 0.1347\n",
            "Epoch [200/300], Loss: 0.1338\n",
            "Epoch [201/300], Loss: 0.1357\n",
            "Epoch [202/300], Loss: 0.1352\n",
            "Epoch [203/300], Loss: 0.1364\n",
            "Epoch [204/300], Loss: 0.1358\n",
            "Epoch [205/300], Loss: 0.1331\n",
            "Epoch [206/300], Loss: 0.1347\n",
            "Epoch [207/300], Loss: 0.1336\n",
            "Epoch [208/300], Loss: 0.1353\n",
            "Epoch [209/300], Loss: 0.1343\n",
            "Epoch [210/300], Loss: 0.1364\n",
            "Epoch [211/300], Loss: 0.1348\n",
            "Epoch [212/300], Loss: 0.1341\n",
            "Epoch [213/300], Loss: 0.1339\n",
            "Epoch [214/300], Loss: 0.1350\n",
            "Epoch [215/300], Loss: 0.1343\n",
            "Epoch [216/300], Loss: 0.1326\n",
            "Epoch [217/300], Loss: 0.1354\n",
            "Epoch [218/300], Loss: 0.1351\n",
            "Epoch [219/300], Loss: 0.1344\n",
            "Epoch [220/300], Loss: 0.1347\n",
            "Epoch [221/300], Loss: 0.1345\n",
            "Epoch [222/300], Loss: 0.1335\n",
            "Epoch [223/300], Loss: 0.1324\n",
            "Epoch [224/300], Loss: 0.1320\n",
            "Epoch [225/300], Loss: 0.1348\n",
            "Epoch [226/300], Loss: 0.1331\n",
            "Epoch [227/300], Loss: 0.1322\n",
            "Epoch [228/300], Loss: 0.1326\n",
            "Epoch [229/300], Loss: 0.1339\n",
            "Epoch [230/300], Loss: 0.1344\n",
            "Epoch [231/300], Loss: 0.1333\n",
            "Epoch [232/300], Loss: 0.1324\n",
            "Epoch [233/300], Loss: 0.1338\n",
            "Epoch [234/300], Loss: 0.1336\n",
            "Epoch [235/300], Loss: 0.1345\n",
            "Epoch [236/300], Loss: 0.1336\n",
            "Epoch [237/300], Loss: 0.1335\n",
            "Epoch [238/300], Loss: 0.1330\n",
            "Epoch [239/300], Loss: 0.1333\n",
            "Epoch [240/300], Loss: 0.1329\n",
            "Epoch [241/300], Loss: 0.1320\n",
            "Epoch [242/300], Loss: 0.1344\n",
            "Epoch [243/300], Loss: 0.1339\n",
            "Epoch [244/300], Loss: 0.1345\n",
            "Epoch [245/300], Loss: 0.1334\n",
            "Epoch [246/300], Loss: 0.1352\n",
            "Epoch [247/300], Loss: 0.1335\n",
            "Epoch [248/300], Loss: 0.1322\n",
            "Epoch [249/300], Loss: 0.1330\n",
            "Epoch [250/300], Loss: 0.1328\n",
            "Epoch [251/300], Loss: 0.1334\n",
            "Epoch [252/300], Loss: 0.1329\n",
            "Epoch [253/300], Loss: 0.1319\n",
            "Epoch [254/300], Loss: 0.1324\n",
            "Epoch [255/300], Loss: 0.1325\n",
            "Epoch [256/300], Loss: 0.1324\n",
            "Epoch [257/300], Loss: 0.1329\n",
            "Epoch [258/300], Loss: 0.1329\n",
            "Epoch [259/300], Loss: 0.1332\n",
            "Epoch [260/300], Loss: 0.1324\n",
            "Epoch [261/300], Loss: 0.1324\n",
            "Epoch [262/300], Loss: 0.1301\n",
            "Epoch [263/300], Loss: 0.1338\n",
            "Epoch [264/300], Loss: 0.1320\n",
            "Epoch [265/300], Loss: 0.1361\n",
            "Epoch [266/300], Loss: 0.1342\n",
            "Epoch [267/300], Loss: 0.1331\n",
            "Epoch [268/300], Loss: 0.1335\n",
            "Epoch [269/300], Loss: 0.1324\n",
            "Epoch [270/300], Loss: 0.1338\n",
            "Epoch [271/300], Loss: 0.1340\n",
            "Epoch [272/300], Loss: 0.1320\n",
            "Epoch [273/300], Loss: 0.1304\n",
            "Epoch [274/300], Loss: 0.1325\n",
            "Epoch [275/300], Loss: 0.1308\n",
            "Epoch [276/300], Loss: 0.1312\n",
            "Epoch [277/300], Loss: 0.1308\n",
            "Epoch [278/300], Loss: 0.1309\n",
            "Epoch [279/300], Loss: 0.1297\n",
            "Epoch [280/300], Loss: 0.1298\n",
            "Epoch [281/300], Loss: 0.1298\n",
            "Epoch [282/300], Loss: 0.1297\n",
            "Epoch [283/300], Loss: 0.1320\n",
            "Epoch [284/300], Loss: 0.1308\n",
            "Epoch [285/300], Loss: 0.1303\n",
            "Epoch [286/300], Loss: 0.1323\n",
            "Epoch [287/300], Loss: 0.1311\n",
            "Epoch [288/300], Loss: 0.1316\n",
            "Epoch [289/300], Loss: 0.1312\n",
            "Epoch [290/300], Loss: 0.1295\n",
            "Epoch [291/300], Loss: 0.1318\n",
            "Epoch [292/300], Loss: 0.1296\n",
            "Epoch [293/300], Loss: 0.1311\n",
            "Epoch [294/300], Loss: 0.1296\n",
            "Epoch [295/300], Loss: 0.1307\n",
            "Epoch [296/300], Loss: 0.1313\n",
            "Epoch [297/300], Loss: 0.1290\n",
            "Epoch [298/300], Loss: 0.1301\n",
            "Epoch [299/300], Loss: 0.1314\n",
            "Epoch [300/300], Loss: 0.1303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for k, linear1 in enumerate(model.linears1):\n",
        "  if k > 0:\n",
        "    for j, param in enumerate(linear1.parameters()):\n",
        "      param.data = list(model.linears1[0].parameters())[j].data\n",
        "\n",
        "for k, linear2 in enumerate(model.linears2):\n",
        "  if k > 0:\n",
        "    for i, param in enumerate(linear2.parameters()):\n",
        "      param.data = list(model.linears2[0].parameters())[i].data\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\"\"\"\n",
        "num_epochs = 3\n",
        "\n",
        "model.freeze_generic_parts()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (idxs, dists, ground_truths, noisy_series) in enumerate(dataloader):\n",
        "        inputs, targets = noisy_series.to(device), \\\n",
        "                          ground_truths.to(device)\n",
        "\n",
        "        # Add the channel's dim\n",
        "        inputs = inputs.unsqueeze(1)\n",
        "        targets = targets.unsqueeze(1)\n",
        "\n",
        "        # Zeroing gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, idxs.to(device))\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')"
      ],
      "metadata": {
        "id": "CArXAsdOTOtG",
        "outputId": "82151f10-e54d-4f49-b7ba-4dcea2c7e0c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CArXAsdOTOtG",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Loss: 0.1254\n",
            "Epoch [2/3], Loss: 0.1262\n",
            "Epoch [3/3], Loss: 0.1262\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}